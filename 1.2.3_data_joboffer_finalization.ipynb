{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOB OFFER'S DATA (part 3: cleaning and transforming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SETTING UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nessesary libraries.\n",
    "import pandas as pd\n",
    "import re\n",
    "from random import randint\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global variables.\n",
    "label_value = ['registered_nurse', 'electrician', 'data_analyst', 'registered_nurse', 'electrician', 'data_analyst']\n",
    "csv_file_name = ['data_jobads_rn.csv', 'data_jobads_e.csv', 'data_jobads_da.csv', 'data_jobads_rn_20jan.csv', 'data_jobads_e_20jan.csv', 'data_jobads_da_20jan.csv']\n",
    "data_frame = ['df1', 'df2', 'df3', 'df4', 'df5', 'df6']\n",
    "to_remove = ['salary', 'schedule', 'benefit', 'location', 'job type', 'office', 'tag', 'employment type', 'email', 'ref.no', \n",
    "             'contact name', 'job ref', 'offer in return', 'job title', 'received by', 'signature date', '______', 'block capitals']\n",
    "date_of_download = ['January 10, 2024', 'January 20, 2024']\n",
    "before_30_days = ['before December 11, 2023', 'before December 21, 2023']\n",
    "directory = r'C:\\Users\\temulenbd\\OneDrive\\Desktop\\learn\\github_repo\\cct\\capstone_project'\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to import a CSV file, add a label column, and return the DataFrame.\n",
    "def import_and_label(csv, value):\n",
    "    \n",
    "    # Declare global variables.\n",
    "    global df\n",
    "    \n",
    "    # Read the CSV file into a DataFrame.\n",
    "    new_df = pd.read_csv(csv, index_col=None)\n",
    "    \n",
    "    # Add a new column 'label' with the specified value.\n",
    "    new_df['label'] = value\n",
    "    \n",
    "    # Concatenate the existing DataFrame 'df' with the new DataFrame.\n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    print('The values of the <'+ csv + '> file were successfully added to the <df>.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to replace 'Just posted' or 'Today' with 'January 10, 2024'.\n",
    "def replace_just_posted_10th(date):\n",
    "    global date_of_download\n",
    "    date_download = date_of_download[0]\n",
    "    return date_download if 'Just posted' in str(date) or 'Today' in str(date) else date\n",
    "\n",
    "# Define a custom function to replace 'Just posted' or 'Today' with 'January 20, 2024'.\n",
    "def replace_just_posted_20th(date):\n",
    "    global date_of_download\n",
    "    date_download = date_of_download[1]\n",
    "    return date_download if 'Just posted' in str(date) or 'Today' in str(date) else date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to extract and replace values with relevant information.\n",
    "def remove_replace_elements_10th(date):\n",
    "    global date_of_download\n",
    "    if date and ('day' in str(date) or 'days' in str(date)):\n",
    "        days_ago = int(re.search(r'(\\d+) (?:day|days) ago', str(date)).group(1))\n",
    "        new_date = datetime.strptime(date_of_download[0], '%B %d, %Y') - timedelta(days=days_ago)\n",
    "        return new_date.strftime('%B %d, %Y')\n",
    "    else:\n",
    "        return date\n",
    "    \n",
    "# Define a custom function extract and replace with relevant information.    \n",
    "def remove_replace_elements_20th(date):\n",
    "    global date_of_download\n",
    "    if date and ('day' in str(date) or 'days' in str(date)):\n",
    "        days_ago = int(re.search(r'(\\d+) (?:day|days) ago', str(date)).group(1))\n",
    "        new_date = datetime.strptime(date_of_download[1], '%B %d, %Y') - timedelta(days=days_ago)\n",
    "        return new_date.strftime('%B %d, %Y')\n",
    "    else:\n",
    "        return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the costum function to find and remove unnecessary or private information.\n",
    "def remove_elements_with_colon(column_value, remove_value):\n",
    "        \n",
    "    # Convert the value to lowercase.\n",
    "    column_value = column_value.lower()\n",
    "    \n",
    "    # Split each value of the column into a list using '\\n' as a separator.\n",
    "    elements = column_value.split('\\n')\n",
    "    \n",
    "    # Find the index of the element containing given value.\n",
    "    index_of_element = next((i for i, elements in enumerate(elements) if remove_value in elements and ':' in elements), None)\n",
    "    \n",
    "    # Check if the first conditions are present in the text.\n",
    "    if index_of_element is not None:\n",
    "        next_colon_index = next((j for j in range(index_of_element + 1, len(elements)) if ':' in elements[j]), None)\n",
    "        \n",
    "        # Add an extra condition to check the second conditions are present in the text.\n",
    "        if next_colon_index is not None:\n",
    "            del elements[index_of_element:next_colon_index]\n",
    "            return '\\n'.join(elements)\n",
    "        else:\n",
    "            return column_value\n",
    "        \n",
    "    else:\n",
    "        return column_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the costum function to find and remove unnecessary or private information.\n",
    "def remove_unnec_lines(column_value, remove_value):\n",
    "    # Split each value of the column into a list using '\\n' as a separator.\n",
    "    elements = column_value.split('\\n')\n",
    "\n",
    "    # Function to check if a line contains an email address, phone number, or link\n",
    "    def is_unwanted_line(line):\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        phone_pattern = r'\\b\\d+[-.\\s+]?\\d+[-.\\s+]?\\d+\\b'\n",
    "        link_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "        return re.search(email_pattern, line) or re.search(phone_pattern, line) or re.search(link_pattern, line)\n",
    "\n",
    "    # Remove lines containing the specified remove_value\n",
    "    index_of_element = next((i for i, element in enumerate(elements) if remove_value in element), None)\n",
    "    if index_of_element is not None:\n",
    "        elements.pop(index_of_element)\n",
    "\n",
    "    # Remove lines containing emails, phone numbers, and links\n",
    "    elements = [element for element in elements if not is_unwanted_line(element)]\n",
    "\n",
    "    return '\\n'.join(elements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CLEANING AND TRANSFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**january 10, 2024**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values of the <data_jobads_rn.csv> file were successfully added to the <df>.\n",
      "The values of the <data_jobads_e.csv> file were successfully added to the <df>.\n",
      "The values of the <data_jobads_da.csv> file were successfully added to the <df>.\n",
      "--------------------------------\n",
      "The newly created <df> has 1034 rows and 6 columns.\n"
     ]
    }
   ],
   "source": [
    "# Merge csv file from 10th of January into one DataFrame.\n",
    "for x in range(3):\n",
    "    import_and_label(csv_file_name[x], label_value[x])\n",
    "\n",
    "print('--------------------------------')\n",
    "rows = df.shape[0]\n",
    "columns = df.shape[1]\n",
    "print(f'The newly created <df> has {rows} rows and {columns} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 114 duplicate rows were removed from the <df>\n",
      "Now, the <df> contains 920 rows and 6 columns.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows based on the 'id' column and keep the original rows.\n",
    "df = df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "print(f'There are {rows-df.shape[0]} duplicate rows were removed from the <df>')\n",
    "print(f'Now, the <df> contains {df.shape[0]} rows and {df.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before December 11, 2023    425\n",
      "unknown                      72\n",
      "January 08, 2024             71\n",
      "December 20, 2023            44\n",
      "January 05, 2024             40\n",
      "December 22, 2023            37\n",
      "January 10, 2024             30\n",
      "January 09, 2024             28\n",
      "January 03, 2024             28\n",
      "January 04, 2024             21\n",
      "January 06, 2024             18\n",
      "December 13, 2023            14\n",
      "December 19, 2023            12\n",
      "December 14, 2023            10\n",
      "December 21, 2023            10\n",
      "December 23, 2023             8\n",
      "December 12, 2023             8\n",
      "December 16, 2023             7\n",
      "December 30, 2023             6\n",
      "January 02, 2024              6\n",
      "December 15, 2023             5\n",
      "January 07, 2024              5\n",
      "December 29, 2023             3\n",
      "December 18, 2023             2\n",
      "January 01, 2024              2\n",
      "December 28, 2023             2\n",
      "December 26, 2023             2\n",
      "December 31, 2023             1\n",
      "December 11, 2023             1\n",
      "December 24, 2023             1\n",
      "December 27, 2023             1\n",
      "Name: date, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract and replace values of the 'date' column.\n",
    "df['date'] = df['date'].replace(['not available', 'Hiring ongoing', None], 'unknown').replace(['Posted 30+ days ago'], before_30_days[0]).apply(replace_just_posted_10th).apply(remove_replace_elements_10th)\n",
    "\n",
    "# Save the values and reset the <df>.\n",
    "df_10th = df.copy()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Assuming df is your DataFrame and 'date' is the column\n",
    "date_counts = df_10th['date'].value_counts()\n",
    "\n",
    "# Print the counts for each unique value in the 'date' column\n",
    "print(date_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**january 20, 2024**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values of the <data_jobads_rn_20jan.csv> file were successfully added to the <df>.\n",
      "The values of the <data_jobads_e_20jan.csv> file were successfully added to the <df>.\n",
      "The values of the <data_jobads_da_20jan.csv> file were successfully added to the <df>.\n",
      "--------------------------------\n",
      "The newly created <df> has 333 rows and 6 columns.\n"
     ]
    }
   ],
   "source": [
    "# Merge all csv file into one DataFrame\n",
    "for x in range(3, 6):\n",
    "    import_and_label(csv_file_name[x], label_value[x])\n",
    "\n",
    "print('--------------------------------')\n",
    "rows = df.shape[0]\n",
    "columns = df.shape[1]\n",
    "print(f'The newly created <df> has {rows} rows and {columns} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14 duplicate rows were removed from the <df>\n",
      "Now, the <df> contains 319 rows and 6 columns.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows based on the 'id' column and keep the original rows.\n",
    "df = df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "print(f'There are {rows-df.shape[0]} duplicate rows were removed from the <df>')\n",
    "print(f'Now, the <df> contains {df.shape[0]} rows and {df.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "January 18, 2024            61\n",
      "unknown                     40\n",
      "before December 21, 2023    35\n",
      "January 20, 2024            32\n",
      "January 17, 2024            30\n",
      "January 19, 2024            29\n",
      "January 16, 2024            26\n",
      "January 12, 2024            19\n",
      "January 13, 2024            14\n",
      "January 11, 2024             9\n",
      "January 10, 2024             8\n",
      "January 15, 2024             7\n",
      "January 09, 2024             3\n",
      "January 14, 2024             1\n",
      "December 22, 2023            1\n",
      "January 06, 2024             1\n",
      "January 02, 2024             1\n",
      "January 03, 2024             1\n",
      "December 23, 2023            1\n",
      "Name: date, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract and replace values of the 'date' column.\n",
    "df['date'] = df['date'].replace(['not available', 'Hiring ongoing', None], 'unknown').replace(['Posted 30+ days ago'], before_30_days[1]).apply(replace_just_posted_20th).apply(remove_replace_elements_20th)\n",
    "\n",
    "# Save the values and reset the <df>.\n",
    "df_20th = df.copy()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Assuming df is your DataFrame and 'date' is the column\n",
    "date_counts = df_20th['date'].value_counts()\n",
    "\n",
    "# Print the counts for each unique value in the 'date' column\n",
    "print(date_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**combining and finalizing the process in merged data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The newly created <df> has 1239 rows and 6 columns.\n"
     ]
    }
   ],
   "source": [
    "# Merge two dataframes into one.\n",
    "df = pd.concat([df_10th, df_20th], ignore_index=True)\n",
    "rows = df.shape[0]\n",
    "columns = df.shape[1]\n",
    "\n",
    "print(f'The newly created <df> has {rows} rows and {columns} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 duplicate rows were removed from the <df>\n",
      "Now, the <df> contains 1239 rows and 6 columns.\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows based on the 'id' column and keep the original rows.\n",
    "df = df.drop_duplicates(subset='link', keep='first')\n",
    "\n",
    "print(f'There are {rows-df.shape[0]} duplicate rows were removed from the <df>')\n",
    "print(f'Now, the <df> contains {df.shape[0]} rows and {df.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM EXAMPLES:\n",
      "\n",
      "type:\n",
      "permanent\n",
      "our client, a premier facilities management company, provides tailed integrated fm services and is currently seeking a full-time facilities electrician f a permanent position suppt a client site in cabinteely.\n",
      "as a leading provider of facilities management services in ireland, europe, and the uk, our client offers a dynamic wk environment where your expertise contributes to the creation of exceptional wkspaces f clients.\n",
      "wking model: onsite client site in cabinteely\n",
      "visa sponsship: no\n",
      "key responsibilities:\n",
      "provide exemplary customer service, ensuring satisfaction at all times.\n",
      "adhere to and lead company health and safety policies and guidelines.\n",
      "comply with and implement existing business operation procedures.\n",
      "take complete ownership of your w\n",
      "king environment and systems, ensuring optimal functioning.\n",
      "ensure service delivery meets\n",
      "exceeds sla requirements.\n",
      "manage and ensure timely completion of helpdesk tasks.\n",
      "maintain high standards of housekeeping throughout the building.\n",
      "keep all rams up to date befe commencing w\n",
      "ks on site.\n",
      "maintain accurate recds f all services perf\n",
      "med.\n",
      "rep\n",
      "t malfunctions to the facilities team and implement effective solutions.\n",
      "safely and efficiently install, maintain, and repair plant and equipment as required.\n",
      "conduct routine planned preventative maintenance (ppm) and commissioning of plant and equipment.\n",
      "perf\n",
      "m electrical diagnostics and fault finding on plant and equipment.\n",
      "undertake c\n",
      "rective maintenance as necessary.\n",
      "conduct audits, rounds, and readings on site, taking remedial actions when needed.\n",
      "assist with hvac, mechanical maintenance, and building fabric repairs as required.\n",
      "liaise with facilities and it departments on project w\n",
      "ks.\n",
      "manage all spare parts f\n",
      "your responsible area.\n",
      "take full ownership of energy management and the green agenda on site.\n",
      "w\n",
      "k flexible duties and hours based on requirements and on-call rota.\n",
      "about you:\n",
      "fully qualified electrician with 3+ yearsâ€™ experience in facilities and services critical environment.\n",
      "proven experience & competence in, pat and thermal imaging predictive maintenance.\n",
      "detailed knowledge of ppm systems.\n",
      "proven competence of emergency lighting and fire protection and detection systems\n",
      "basic handyman, building maintenance skills.\n",
      "basic knowledge / understanding of hvac systems.\n",
      "irish full driving licence.\n",
      "a pension contribution of up to 5%.\n",
      "provision of a company-branded vehicle.\n",
      "fuel card f\n",
      "convenience.\n",
      "complimentary gp services f\n",
      "both you and your family.\n",
      "enjoy 25 days of annual leave f\n",
      "your leisure.\n",
      "submit your cv (in a microsoft wd fmat) today!\n",
      "\n",
      "---------\n",
      "type permanent\n",
      "start asap\n",
      "business analyst\n",
      "desired skills:\n",
      "business analyst, finance, payments, data analysis\n",
      "hybrid\n",
      "a fintech company with a great product are hiring a business analyst with a strong background in data. you will work to provide technical solutions and documentation through the use of data analytics, develop and implement internal systems and map/automate new business processes.\n",
      "requirements:\n",
      "at least 3 years in a business analyst role, with a background/experience in data analytics\n",
      "experience with data warehousing, sql, data models and using technical insights to influence informed business decisions\n",
      "fintech experience preferred, but not essential\n",
      "apply via the link , or to register your interest in similar positions:\n",
      "\n",
      "reperio human capital acts as an employment agency and an employment business.\n",
      "fergal wiseman is recruiting for this role.\n"
     ]
    }
   ],
   "source": [
    "# Remove any private and unnecessary information from the job details.\n",
    "for val_rem in to_remove:\n",
    "    df['job_description'] = df['job_description'].apply(lambda x: remove_elements_with_colon(x, val_rem)\n",
    "                                                        ).apply(lambda x:remove_unnec_lines(x, val_rem))\n",
    "\n",
    "print('RANDOM EXAMPLES:\\n')\n",
    "print(df.iat[randint(0, 1034), 4])\n",
    "print('---------')\n",
    "print(df.iat[randint(0, 1034), 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create the file path for CSV export.\n",
    "file_path = os.path.join(directory, 'data_jobads_final.csv')\n",
    "    \n",
    "# Export the DataFrame to CSV file.\n",
    "df.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
